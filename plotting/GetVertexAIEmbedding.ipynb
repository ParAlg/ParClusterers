{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65ff1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "# need to run gcloud auth application-default login\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "vertexai.init(project=\"julian-shun-3fef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "413f70b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Embedding Vector: 768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"Text embedding with a Large Language Model.\"\"\"\n",
    "inputs = []\n",
    "for text in [\"w\" * 1000000]:\n",
    "    text_embedding_input = TextEmbeddingInput(\n",
    "        task_type=\"CLUSTERING\", text=text)\n",
    "    inputs.append(text_embedding_input)\n",
    "embeddings = model.get_embeddings(inputs)\n",
    "all_vectors = []\n",
    "for embedding in embeddings:\n",
    "    vector = embedding.values\n",
    "    all_vectors.append(vector)\n",
    "    print(f\"Length of Embedding Vector: {len(vector)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0df5166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76664754]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([all_vectors_old[0]], [all_vectors_old[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56d6f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"embed data from http://manikvarma.org/downloads/XC/XMLRepository.html\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "import gzip\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "base_dir = \"../xc_data\"\n",
    "\n",
    "def read_corpus(fname):\n",
    "    \"\"\"\n",
    "    Read gzip file with one json string in each line\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A generator with dictionary as values\n",
    "    \"\"\"\n",
    "    with gzip.open(fname, \"r\") as fp:\n",
    "        for line in fp:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "def read(fname, title_only):\n",
    "    \"\"\"\n",
    "    if title_only is True, we only return the titles. Otherwise concat title and content.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    text = []\n",
    "    fp = read_corpus(fname)\n",
    "    for line in fp:\n",
    "        # different dataset might use different fields\n",
    "        if title_only:\n",
    "            text.append(line[\"title\"])\n",
    "        else:\n",
    "            text.append(line[\"title\"] + line[\"content\"])\n",
    "        labels.append(line[\"target_ind\"])\n",
    "    return text, labels\n",
    "\n",
    "\n",
    "def get_communities(labels):\n",
    "    \"\"\"\n",
    "    `labels[i]` is the label for node i\n",
    "    Returns a list of list where each inner list is a community. Duplicated communities are removed.\n",
    "    \"\"\"\n",
    "    communities = defaultdict(set)\n",
    "    for i, clusters in enumerate(labels):\n",
    "        for c in clusters:\n",
    "            communities[c].add(i)\n",
    "    # Convert each inner list into a tuple and create a set of tuples\n",
    "    unique_set = set(tuple(inner_list) for inner_list in communities.values())\n",
    "\n",
    "    # Convert the set of tuples back into a list of lists\n",
    "    unique_list = [list(inner_tuple) for inner_tuple in unique_set]\n",
    "    return unique_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "980004c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(10))\n",
    "def get_embeddings(texts, model):\n",
    "    \"\"\"\n",
    "    Fetches embeddings for a batch of texts using a specified OpenAI embedding model.\n",
    "\n",
    "    Args:\n",
    "    - texts (list of str): The texts to encode.\n",
    "    - model (str): The model to use for encoding.\n",
    "\n",
    "    Returns:\n",
    "    - list of list: A list of embedding lists, each representing the embedding for a text.\n",
    "    \"\"\"\n",
    "    embedding_model = TextEmbeddingModel.from_pretrained(model)\n",
    "    inputs = []\n",
    "    for text in texts:\n",
    "        text_embedding_input = TextEmbeddingInput(\n",
    "            task_type=\"CLUSTERING\", text=text)\n",
    "        inputs.append(text_embedding_input)\n",
    "    embeddings = embedding_model.get_embeddings(inputs)\n",
    "    all_vectors = []\n",
    "    for embedding in embeddings:\n",
    "        vector = embedding.values\n",
    "        all_vectors.append(vector)\n",
    "        return all_vectors\n",
    "\n",
    "\n",
    "def embed_texts(text, batch_size=2000):\n",
    "    \"\"\"\n",
    "    `text` is a list of strings, return the embeddings for each string in the list.\n",
    "    \"\"\"\n",
    "    n = len(text)\n",
    "    embeddings = []\n",
    "\n",
    "    for start in tqdm.tqdm(range(0, n, batch_size)):\n",
    "        end = start + batch_size\n",
    "        batch_texts = text[start:end]\n",
    "        batch_embeddings = get_embeddings(batch_texts, model=\"textembedding-gecko@003\")\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_vectors_and_labels(train_dir, test_dir, title_only):\n",
    "    token_limit =  8192\n",
    "    encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "    all_labels = []\n",
    "    all_embeddings = []\n",
    "    for data_dir in [train_dir, test_dir]:\n",
    "        text, labels = read(data_dir, title_only)\n",
    "        all_labels.extend(labels)\n",
    "        num_tokens = 0\n",
    "        text = text[:300] # TODO:remove\n",
    "        for i in tqdm.tqdm(range(len(text)), \"check_encoding_length\"):\n",
    "            tokens = encoding.encode(text[i])\n",
    "            if len(tokens) > token_limit:\n",
    "                new_text =  encoding.decode(tokens[:token_limit])\n",
    "                text[i] = new_text\n",
    "            num_tokens += min(token_limit, len(tokens))\n",
    "                \n",
    "        print(\"price: \", num_tokens/1000 * 0.0002)\n",
    "\n",
    "        embeddings = embed_texts(text, batch_size=1)\n",
    "        all_embeddings.extend(embeddings)\n",
    "    communities = get_communities(all_labels)\n",
    "    print(\"num communities\", len(communities))\n",
    "    return all_embeddings, communities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "17841449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon-670K.raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_encoding_length: 100%|████████████████████| 300/300 [00:00<00:00, 6687.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price:  0.0202892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 300/300 [02:35<00:00,  1.93it/s]\n",
      "check_encoding_length: 100%|███████████████████| 300/300 [00:00<00:00, 11449.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price:  0.012268800000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                       | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     train_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/trn.raw.json.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     test_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tst.raw.json.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m all_embeddings, communities \u001b[38;5;241m=\u001b[39m \u001b[43mget_vectors_and_labels\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle_only_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_embeddings)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikipedia\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m title_only_dict[dataset]:\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36mget_vectors_and_labels\u001b[0;34m(train_dir, test_dir, title_only)\u001b[0m\n\u001b[1;32m     57\u001b[0m         num_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(token_limit, \u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice: \u001b[39m\u001b[38;5;124m\"\u001b[39m, num_tokens\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.0002\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m     63\u001b[0m communities \u001b[38;5;241m=\u001b[39m get_communities(all_labels)\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36membed_texts\u001b[0;34m(text, batch_size)\u001b[0m\n\u001b[1;32m     35\u001b[0m     end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m batch_size\n\u001b[1;32m     36\u001b[0m     batch_texts \u001b[38;5;241m=\u001b[39m text[start:end]\n\u001b[0;32m---> 37\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtextembedding-gecko@003\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend(batch_embeddings)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tenacity/__init__.py:324\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tenacity/__init__.py:404\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tenacity/__init__.py:349\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    347\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m retry_state\u001b[38;5;241m.\u001b[39moutcome\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(retry_state\u001b[38;5;241m.\u001b[39moutcome\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state\u001b[38;5;241m=\u001b[39mretry_state)):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tenacity/__init__.py:407\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    409\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(texts, model)\u001b[0m\n\u001b[1;32m     16\u001b[0m     text_embedding_input \u001b[38;5;241m=\u001b[39m TextEmbeddingInput(\n\u001b[1;32m     17\u001b[0m         task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLUSTERING\u001b[39m\u001b[38;5;124m\"\u001b[39m, text\u001b[38;5;241m=\u001b[39mtext)\n\u001b[1;32m     18\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(text_embedding_input)\n\u001b[0;32m---> 19\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m all_vectors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m embeddings:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/vertexai/language_models/_language_models.py:2061\u001b[0m, in \u001b[0;36mTextEmbeddingModel.get_embeddings\u001b[0;34m(self, texts, auto_truncate)\u001b[0m\n\u001b[1;32m   2047\u001b[0m \u001b[38;5;124;03m\"\"\"Calculates embeddings for the given texts.\u001b[39;00m\n\u001b[1;32m   2048\u001b[0m \n\u001b[1;32m   2049\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;124;03m    A list of `TextEmbedding` objects.\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m prediction_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_text_embedding_request(\n\u001b[1;32m   2057\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m   2058\u001b[0m     auto_truncate\u001b[38;5;241m=\u001b[39mauto_truncate,\n\u001b[1;32m   2059\u001b[0m )\n\u001b[0;32m-> 2061\u001b[0m prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2066\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(prediction_response\u001b[38;5;241m.\u001b[39mpredictions)):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/google/cloud/aiplatform/models.py:1579\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1566\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1567\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mjson_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1576\u001b[0m         ),\n\u001b[1;32m   1577\u001b[0m     )\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1579\u001b[0m     prediction_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata:\n\u001b[1;32m   1586\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m json_format\u001b[38;5;241m.\u001b[39mMessageToDict(prediction_response\u001b[38;5;241m.\u001b[39m_pb\u001b[38;5;241m.\u001b[39mmetadata)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:836\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 836\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/google/api_core/grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/grpc/_channel.py:1173\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1163\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1169\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1170\u001b[0m     (\n\u001b[1;32m   1171\u001b[0m         state,\n\u001b[1;32m   1172\u001b[0m         call,\n\u001b[0;32m-> 1173\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/grpc/_channel.py:1157\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1141\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[1;32m   1142\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m   1143\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context,\n\u001b[1;32m   1156\u001b[0m )\n\u001b[0;32m-> 1157\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:367\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:188\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:182\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/credentials.pyx.pxi:62\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._get_metadata\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/credentials.pyx.pxi:28\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._spawn_callback_async\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/credentials.pyx.pxi:17\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._spawn_callback_in_thread\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/fork_posix.pyx.pxi:112\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.ForkManagedThread.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/threading.py:795\u001b[0m, in \u001b[0;36mThread.__init__\u001b[0;34m(self, group, target, name, args, kwargs, daemon)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;124;03m\"\"\"A class that represents a thread of control.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03mThis class can be safely subclassed in a limited fashion. There are two ways\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m \n\u001b[1;32m    791\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    793\u001b[0m _initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    796\u001b[0m              args\u001b[38;5;241m=\u001b[39m(), kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;124;03m\"\"\"This constructor should always be called with keyword arguments. Arguments are:\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \n\u001b[1;32m    799\u001b[0m \u001b[38;5;124;03m    *group* should be None; reserved for future extension when a ThreadGroup\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    816\u001b[0m \n\u001b[1;32m    817\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup argument must be None for now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_dir = \"/Users/sy/Desktop/MIT/clusterer/data/\"\n",
    "\n",
    "datasets = [\n",
    "    \"AmazonTitles-670K\",\n",
    "#     \"WikiSeeAlsoTItles-350K\",\n",
    "#     \"Amazon-670K.raw\",\n",
    "#     \"Wikipedia-500K.raw\",\n",
    "]\n",
    "title_only_dict = {\n",
    "    \"AmazonTitles-670K\": True,\n",
    "    \"WikiSeeAlsoTItles-350K\": True,\n",
    "    \"Amazon-670K.raw\": False,\n",
    "    \"Wikipedia-500K.raw\": True,\n",
    "}\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    train_dir = f\"{base_dir}/{dataset}/trn.json.gz\"\n",
    "    test_dir = f\"{base_dir}/{dataset}/tst.json.gz\"\n",
    "    if \".raw\" in dataset:\n",
    "        train_dir = f\"{base_dir}/{dataset}/trn.raw.json.gz\"\n",
    "        test_dir = f\"{base_dir}/{dataset}/tst.raw.json.gz\"\n",
    "\n",
    "    all_embeddings, communities = get_vectors_and_labels(\n",
    "        train_dir, test_dir, title_only_dict[dataset]\n",
    "    )\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "\n",
    "    if dataset.startswith(\"Wikipedia\") and title_only_dict[dataset]:\n",
    "        dataset = \"WikiTitles\"\n",
    "    dataset = dataset.split(\"-\")[0]\n",
    "    print(\"saving to, \", f\"{base_dir}/{dataset}_palm.npy\")\n",
    "    with open(f\"{base_dir}/{dataset}_palm.npy\", \"wb\") as f:\n",
    "        np.save(f, all_embeddings)\n",
    "\n",
    "    lines_to_write = []\n",
    "    for cluster_list in communities:\n",
    "        lines_to_write.append(\"\\t\".join(str(x) for x in cluster_list))\n",
    "\n",
    "    print(\"saving to, \", f\"{base_dir}/{dataset}_palm.cmty\")\n",
    "    with open(f\"{base_dir}/{dataset}_palm.cmty\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines_to_write) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d702ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
